+++
title = "Product and Support Knowledge Base"
date = 2025
weight = 3
chapter = false
pre = "<b>3. </b>"
+++

### Background
After uncovering key initial findings in the data using descriptive analytics in QuickSight, lets leverage generative AI capabilities to augment the data discovery process. To ensure a smooth transition during the merger, understanding pain points customers experience with products is paramount. We will be using a combination of AWS generative AI services and prompt engineering best practices to generate outputs.

### Prompt Engineering

Prompt engineering is a technique used in the field of artificial intelligence (AI), particularly with generative AI models such as Large Language Models (LLMs), to craft inputs (prompts) that guide the model to produce desired outputs. It involves designing, refining, and testing prompts to effectively communicate the task or question to the model in a way that maximizes the likelihood of receiving accurate, relevant, and coherent responses.

By applying best practices in engineering prompts used in the knowledge base, we can receive optimal high-value outputs. Additionally, effective prompt engineering requires an understanding of the model's training data, capabilities, and limitations. Prompts must be specific enough to elicit the desired response but flexible enough to allow the model to generate creative or insightful outputs.

### Product and Support Data

You have been provided product and support data for both Ohmzio and Ampwerks. The data is currently stored in an Amazon S3 Bucket.

    s3://customerdata
    └── data/
        ├── ampwerks/
        │   ├── productinfo/
        │   │   └── <product-documentation>.docx
        │   └── support/
        │       └── <support-tickets>.json
        └── ohmzio/
            ├── productinfo/
            │   └── <product-documentation>.docx
            └── support/
                └── <support-tickets>.json

For each respective company, the productinfo/ directory contains .docx files for each product they sell. Each document contain comprehensive descriptions of the product, instructions, and FAQs.

The support/ directory contains .json files for every customer support ticket. These support tickets contain the interations between a support representative and the customer requesting for support about a product they purchased.

The productinfo/ and support/ data will provide highly-relevent contextual data to the foundational model. This will enable the model to provide responses based on real interactions and reduce hallucinations.

{{%notice%}}
**hallucination**: (related to artifical intelligence) is a response generated by AI which contains false or misleading information presented as fact.
{{%/notice%}}

### Amazon Bedrock: Knowledge Bases

To augment discovery efforts, you will use Amazon Bedrock: Knowledge Bases to create a highly specified data source (product and support data for both companies) to perform Retrieval Augmented Generation (RAG) with a Foundation Model (FM) on the data source we define. Here is a logical architecture of the RAG architecture using Knowledge Bases and key concepts:

![1](../images/3/1.png)

* **Retrieval Augmented Generation (RAG)** is a methodology in artificial intelligence that combines the capabilities of two main components: a retrieval system and a generative model. This approach is particularly used in the context of natural language processing (NLP) tasks, such as question answering, content generation, and conversation systems. The objective of RAG is to enhance the quality, relevance, and accuracy of the generative model's outputs by dynamically incorporating external knowledge retrieved in real-time.

* **Large Language Model (LLM)** is a very large deep learning model that is pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. This allows LLMs to be incredibly flexible. One model can perform completely different tasks such as answering questions, summarizing documents, translating languages and completing sentences.

* **Foundational Model (FM)** is broader and encompasses not just language models but any type of deep learning model that is pre-trained on a large dataset and can be fine-tuned for a variety of tasks. This concept extends beyond text to include models trained on images, audio, and more.

* **Vector Store** in the context of generative AI refers to a database or storage system that holds high-dimensional vectors, which are representations of data items (such as text, images, or audio) in a form that machines can efficiently process and understand. These vectors are typically generated through the process of embedding, where data items are converted into vectors of fixed size, capturing the semantic properties and relationships of the original data in a continuous vector space.

{{%notice%}}
When used with RAG, a generative AI model can query a vector store to retrieve relevant information based on the input query. This information can then be used to generate responses that are both relevant and informed by up-to-date or specific data.
{{%/notice%}}

### Steps

* Create Amazon Bedrock: Knowledge Base
* Test knowledge base
* Use prompt engineering to extract data about products and services